{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pyarrow.parquet as pq\n",
    "import fastparquet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from https://programmer.ink/think/5d45867ef3982.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = pd.read_excel('/home/justin/Data/BigDataCallTracesDataModel-RC7-MLfields.xlsx', header=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# parquet_data_path = Path('/home/justin/Code/ran_arcd/project/data/interim/tigo_parquet')\n",
    "parquet_data_path = Path('/home/justin/Data/cdr_full.gz.parquet')\n",
    "df = pd.read_parquet(parquet_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select relevant categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_category(df, database_name, unique_column):\n",
    "    # Unique to SyBase UMTS\n",
    "    try: \n",
    "        df[unique_column]\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Legacy\n",
    "\"\"\"\n",
    "# Unique to SyBase UMTS\n",
    "database_name = 'SyBase UMTS'\n",
    "unique_column = 'initial_lac'\n",
    "sybase_umts_tup = (database_name, unique_column)\n",
    "\n",
    "# Unique to SyBase GSM\n",
    "database_name = 'SyBase GSM'\n",
    "unique_column = 'start_cell_id_lac'\n",
    "sybase_umts_gsm = (database_name, unique_column)\n",
    "\n",
    "# Unique to SyBase LTE\n",
    "database_name = 'SyBase LTE'\n",
    "unique_column = 'enb_ue_s1ap_id'\n",
    "sybase_umts_lte = (database_name, unique_column)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "BigData Model\n",
    "\"\"\"\n",
    "# Unique to BigData Model\n",
    "database_name = 'Avro Schema Output Name'\n",
    "unique_column = 'interface'\n",
    "avro_out = (database_name, unique_column)\n",
    "\n",
    "# List of database names / unique column pairs\n",
    "# [(database_name, unique_column), ...]\n",
    "database_style_lst = [avro_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check whether valid database type\n",
    "\"\"\"\n",
    "result_list = []\n",
    "for database_marker_tup in database_style_lst:\n",
    "    result_list.append(check_unique_category(df, database_marker_tup[0], database_marker_tup[1]))\n",
    "    \n",
    "database_check_results = [(tup[0], tup[1], result_list[i]) for i, tup in enumerate(database_style_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(database_check_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_model.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get column names to keep as inputs\n",
    "\"\"\"\n",
    "feature_names = ('BIGDATA MODEL', 'Output Avro Schema Name')\n",
    "features_of_interest = ('Nex - TSNG (5.2)', 'Relevant for ML?')\n",
    "targets_column = ('Nex - TSNG (5.2)', 'Label?')\n",
    "\n",
    "# data_model[features_of_interest] = data_model[features_of_interest] == 'Y'\n",
    "\n",
    "# print(features_of_interest)\n",
    "# print(data_model.head())\n",
    "# database_check_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_model[data_model[features_of_interest] == 'Y'][feature_names]\n",
    "targets = data_model[data_model[targets_column] == 'Y'][feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate relevant columns / targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get file loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parquet loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# parquet_data_path = Path('/home/justin/Code/ran_arcd/project/data/interim/tigo_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_dataset = pq.ParquetDataset(parquet_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq_dataset.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spark loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark as spark\n",
    "# df_pd = spark.read.parquet(\"...\").sample(0.1, seed=42).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = \"/home/justin/Code/ran_arcd/project/data/interim/\"\n",
    "checkpoint_path = path_base + \"checkpoint\"\n",
    "table_path_base = path_base + \"tigo_parquet/part.*.parquet\"\n",
    "table_path_base_file = table_path_base\n",
    "print(table_path_base_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = spark.read.parquet(table_path_base_file).count()\n",
    "# test_size = spark.read.parquet(table_path_base_file + \"test\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.parquet(table_path_base_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_count = 10000\n",
    "spark_df = spark_df.repartition(partition_count) # resilient distributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = spark_df.rdd.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result.rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = spark_df.select('*').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Workaround for Arrow issue:\n",
    "underscore_files = [f for f in (os.listdir(table_path_base + \"train\") + \n",
    "    os.listdir(table_path_base + \"test\")) if f.startswith(\"_\")]\n",
    "pq.EXCLUDED_PARQUET_PATHS.update(underscore_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_size = 299\n",
    "\n",
    "def transform_reader(reader, batch_size):\n",
    "    \n",
    "    def transform_input(x):\n",
    "        img_bytes = tf.reshape(decode_raw(x.image, tf.uint8), (-1,img_size,img_size,3))\n",
    "        inputs = preprocess_input(tf.cast(img_bytes, tf.float32))\n",
    "        outputs = x.label - 1\n",
    "        return (inputs, outputs)\n",
    "    \n",
    "    return make_petastorm_dataset(reader).map(transform_input).\\\n",
    "        apply(unbatch()).shuffle(400, seed=42).\\\n",
    "        batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.azuredatabricks.net/_static/notebooks/deep-learning/petastorm.html\n",
    "from petastorm import make_batch_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load using pyarrow \n",
    "\"\"\"\n",
    "with make_batch_reader(petastorm_dataset_url, num_epochs=100) as reader:\n",
    "    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1, 28, 28, 1]), tf.one_hot(x.label, 10)))\n",
    "    model = get_model()\n",
    "    optimizer = keras.optimizers.Adadelta()\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(dataset, steps_per_epoch=10, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_minibatches(minibatch_size=1000):\n",
    "    '''\n",
    "    Iterator\n",
    "    Given a file stream (such as a large file), output the minibatch_size line at a time, and select the default line of 1k\n",
    "    Convert output to numpy output, return X, y\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    cur_line_num = 0\n",
    "\n",
    "    train_data, train_label, train_weight, test_data, test_label, test_file = load_data()\n",
    "    train_data, train_label = shuffle(train_data, train_label, random_state=0)  # random_state=0 is used to record the scrambling position to ensure that each scrambling position remains unchanged.\n",
    "    print(type(train_label), train_label)\n",
    "\n",
    "    for data_x, label_y in zip(train_data, train_label):\n",
    "        X.append(data_x)\n",
    "        y.append(label_y)\n",
    "\n",
    "        cur_line_num += 1\n",
    "        if cur_line_num >= minibatch_size:\n",
    "            X, y = np.array(X), np.array(y)  # Converting data to numpy array type and returning\n",
    "            yield X, y\n",
    "            X, y = [], []\n",
    "            cur_line_num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightgbm (LGB) Incremental Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbmTest():\n",
    "    import lightgbm as lgb\n",
    "    # The first step is to initialize the model as None and set the model parameters.\n",
    "    gbm = None\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'application': 'regression',  # objective function\n",
    "        'boosting_type': 'gbdt',  # Setting Upgrade Types\n",
    "        'learning_rate': 0.01,  # Learning rate\n",
    "        'num_leaves': 50,  # Number of leaf nodes\n",
    "        'tree_learner': 'serial',\n",
    "        'min_data_in_leaf': 100,\n",
    "        'metric': ['l1', 'l2', 'rmse'],  # l1:mae, l2:mse  # Evaluation function\n",
    "        'max_bin': 255,\n",
    "        'num_trees': 300\n",
    "    }\n",
    "\n",
    "    # The second step is streaming data (100,000 at a time)\n",
    "    minibatch_train_iterators = iter_minibatches(minibatch_size=10000)\n",
    "\n",
    "    for i, (X_, y_) in enumerate(minibatch_train_iterators):\n",
    "        # Create lgb datasets\n",
    "        # y_ = list(map(float, y_))  # Convert numpy.ndarray to list\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.1, random_state=0)\n",
    "        y_train = y_train.ravel()\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "        # Step 3: Incremental Training Model\n",
    "        # Emphasis is laid on incremental training through init_model and keep_training_booster parameters.\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=1000,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        init_model=gbm,  # If gbm is not None, then it is on the basis of the last training.\n",
    "                        # feature_name=x_cols,\n",
    "                        early_stopping_rounds=10,\n",
    "                        verbose_eval=False,\n",
    "                        keep_training_booster=True)  # Incremental training\n",
    "\n",
    "        print(\"{} time\".format(i))  # Current Number\n",
    "        # Output Model Assessment Score\n",
    "        score_train = dict([(s[1], s[2]) for s in gbm.eval_train()])\n",
    "        print('The score of the current model in the training set is: mae=%.4f, mse=%.4f, rmse=%.4f'\n",
    "              % (score_train['l1'], score_train['l2'], score_train['rmse']))\n",
    "\n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightgbm (LGB) Call Procedure and Save Training Result Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lightgbm Incremental training'''\n",
    "print('lightgbm Incremental training')\n",
    "train_data, train_label, train_weight, test_data, test_label, test_file = load_data()\n",
    "print(train_label.shape,train_data.shape)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_data, train_label, test_size=0.1, random_state=0)\n",
    "# train_X, train_Y = shuffle(train_data, train_label, random_state=0)  # random_state=0 is used to record the scrambling position to ensure that each scrambling position remains unchanged.\n",
    "\n",
    "gbm = lightgbmTest()\n",
    "pred_Y = gbm.predict(test_X)\n",
    "print('compute_loss:{}'.format(compute_loss(test_Y, pred_Y)))\n",
    "\n",
    "# gbm.save_model('lightgbmtest.model')\n",
    "# Model Storage\n",
    "joblib.dump(gbm, 'loan_model.pkl')\n",
    "# Model Loading\n",
    "gbm = joblib.load('loan_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
