{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to Parquet Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet has a much smaller footprint allowing you to reduce storage space and improve performance when loading the data to memory. This transformation is particularly important for out-of-memory computation with increased IO for large datasets. Parquet is column-based storage, making column-based operations particularly effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePath\n",
    "\n",
    "input_directory = \"../data/\"\n",
    "filename = ''\n",
    "extension = '.csv'\n",
    "csv_sep = '\\t'\n",
    "input_file = PurePath(input_directory, filename + extension)\n",
    "\n",
    "output_directory = PurePath(input_directory, filename + '_parquet')\n",
    "output_filename_base = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start local Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting new client\n",
      "<Client: 'tcp://127.0.0.1:36981' processes=5 threads=10, memory=20.00 GB>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36981</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:20100/status' target='_blank'>http://127.0.0.1:20100/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>10</li>\n",
       "  <li><b>Memory: </b>20.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36981' processes=5 threads=10, memory=20.00 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# try:\n",
    "#     if client:\n",
    "#         print('Restarting client')\n",
    "#         client.restart()\n",
    "# except:\n",
    "#     cluster = LocalCluster(dashboard_address=':20100', memory_limit='4G')\n",
    "#     print('Setting new client')\n",
    "#     client = Client(cluster)\n",
    "#     print(client)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(input_file, sep=sep)\n",
    "columns = ddf.columns.values\n",
    "for i, column in enumerate(columns):\n",
    "    print(str(i) + ': ' + column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['']\n",
    "datetime_features = ['']\n",
    "numerical_features = ['']\n",
    "target_columns = ['']\n",
    "\n",
    "# Type dict to improve dynamic loading of csv\n",
    "dtypes = {**{col: 'category' for col in categorical_features}, \\\n",
    "         **{col: 'float64' for col in numerical_features},\\\n",
    "        **{col: 'category' for col in target_columns}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_load = categorical_features + datetime_features + numerical_features + target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(input_file, usecols = columns_to_load, dtype=dtypes, sep=sep, parse_dates = datetime_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide files by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.delayed\n",
    "        \n",
    "def create_parquet_file(ddf, output_filepath):\n",
    "    dd.to_parquet(ddf, output_filepath)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parquet_file(ddf, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance in Dask file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ddf = dd.read_parquet(output_directory, sep=sep)\n",
    "print(test_ddf.head())\n",
    "# output_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet vs CSV reading speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_ddf = dd.read_csv(input_file, usecols = columns_to_load, dtype=dtypes, sep=sep)\n",
    "pq_ddf = dd.read_parquet(output_directory, usecols = columns_to_load, dtype=dtypes, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_result(ddf):\n",
    "    # Have to transform to float64 to prevent overflow and inf outcomes\n",
    "    ddf[''] = ddf[''].astype(np.float64)\n",
    "    return ddf.groupby('')[''].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File size is ~ G\n",
    "%time result = average_result(csv_ddf).compute()    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet directory size ~ G\n",
    "%time result = average_result(pq_ddf).compute()    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
