{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to Parquet Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet has a much smaller footprint allowing you to reduce storage space and improve performance when loading the data to memory. This transformation is particularly important for out-of-memory computation with increased IO for large datasets. Parquet is column-based storage, making column-based operations particularly effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import fastparquet\n",
    "import dask.delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePath\n",
    "\n",
    "input_directory = \"../data/\"\n",
    "filename = '2018_Yellow_Taxi_Trip_Data'\n",
    "extension = '.csv'\n",
    "csv_sep = ','\n",
    "input_file = PurePath(input_directory, filename + extension)\n",
    "\n",
    "output_directory = PurePath(input_directory, filename)\n",
    "output_filename_base = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start local Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting client\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33639</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:20100/status' target='_blank'>http://127.0.0.1:20100/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>10</li>\n",
       "  <li><b>Memory: </b>25.97 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:33639' processes=5 threads=10, memory=25.97 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "try:\n",
    "    if client:\n",
    "        print('Restarting client')\n",
    "        client.restart()\n",
    "except:\n",
    "#     cluster = LocalCluster(dashboard_address=':20100', memory_limit='4G')\n",
    "    cluster = LocalCluster(dashboard_address=':20100')\n",
    "    print('Setting new client')\n",
    "    client = Client(cluster)\n",
    "    print(client)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: VendorID\n",
      "1: tpep_pickup_datetime\n",
      "2: tpep_dropoff_datetime\n",
      "3: passenger_count\n",
      "4: trip_distance\n",
      "5: RatecodeID\n",
      "6: store_and_fwd_flag\n",
      "7: PULocationID\n",
      "8: DOLocationID\n",
      "9: payment_type\n",
      "10: fare_amount\n",
      "11: extra\n",
      "12: mta_tax\n",
      "13: tip_amount\n",
      "14: tolls_amount\n",
      "15: improvement_surcharge\n",
      "16: total_amount\n"
     ]
    }
   ],
   "source": [
    "ddf = dd.read_csv(input_file, sep=csv_sep)\n",
    "columns = ddf.columns.values\n",
    "for i, column in enumerate(columns):\n",
    "    print(str(i) + ': ' + column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['VendorID', \n",
    "                        'RatecodeID', \n",
    "                        'PULocationID',\n",
    "                        'DOLocationID',\n",
    "                        'payment_type',\n",
    "                       ]\n",
    "datetime_features = ['tpep_pickup_datetime',\n",
    "                    'tpep_dropoff_datetime']\n",
    "\n",
    "numerical_features = ['passenger_count', \n",
    "                     'trip_distance', \n",
    "                     'fare_amount', \n",
    "                      'extra',\n",
    "                      'mta_tax',\n",
    "                      'tip_amount',\n",
    "                      'tolls_amount',\n",
    "                      'improvement_surcharge',\n",
    "                      'total_amount'\n",
    "                     ]\n",
    "\n",
    "string_features = ['store_and_fwd_flag']\n",
    "\n",
    "# Type dict to improve dynamic loading of csv\n",
    "dtypes = {**{col: 'category' for col in categorical_features}, \\\n",
    "         **{col: 'float64' for col in numerical_features}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_load = categorical_features + datetime_features + numerical_features + string_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(input_file, \n",
    "                  usecols = columns_to_load, \n",
    "                  dtype=dtypes, \n",
    "                  sep=csv_sep, \n",
    "                  parse_dates = datetime_features, \n",
    "                  blocksize = 32e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row count\n",
    "# print(ddf.shape[0].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf.head()\n",
    "# Transforming with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime columns to a python datetime format\n",
    "# import dateutil.parser as dparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = ddf['tpep_pickup_datetime'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf['tpep_dropoff_datetime'] = ddf['tpep_dropoff_datetime'].apply(dparser.parse, meta='datetime.datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "# single_result = result.iloc[0]\n",
    "# print(type(dparser.parse(single_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map store and fwd flag to boolean N -> False, Y -> True\n",
    "# def bool_conversion(val):\n",
    "#     if val =='T': return True\n",
    "#     elif val == 'N': return False\n",
    "#     else: return None\n",
    "    \n",
    "# df['store_and_fwd_flag'] = df['store_and_fwd_flag'].apply(bool_conversion, meta='bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide files by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_file(ddf, output_filepath):\n",
    "    dd.to_parquet(ddf, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parquet_file(ddf, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance in Dask file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "0   2018-12-03 09:58:01\n",
      "1   2018-12-03 09:41:32\n",
      "2   2018-12-03 08:54:36\n",
      "3   2018-12-03 09:02:08\n",
      "4   2018-12-03 09:10:10\n",
      "Name: tpep_pickup_datetime, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# output_directory = PurePath('datetime_yellow_taxi_parquet')\n",
    "test_ddf = dd.read_parquet(output_directory)\n",
    "print(test_ddf['tpep_pickup_datetime'].head())\n",
    "# output_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet vs CSV reading speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_ddf = dd.read_csv(input_file, usecols = columns_to_load, dtype=dtypes, sep=csv_sep)\n",
    "pq_ddf = dd.read_parquet(output_directory, usecols = columns_to_load, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_result(ddf):\n",
    "    # Have to transform to float64 to prevent overflow and inf outcomes\n",
    "    ddf['trip_distance'] = ddf['trip_distance'].astype(np.float64)\n",
    "    return ddf.groupby('VendorID')['trip_distance'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.88 s, sys: 613 ms, total: 8.49 s\n",
      "Wall time: 1min 10s\n",
      "VendorID\n",
      "1    2.791033\n",
      "2    3.031729\n",
      "4    2.703563\n",
      "Name: trip_distance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# File size is ~ 10Gb\n",
    "%time result = average_result(csv_ddf).compute()    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.84 s, sys: 389 ms, total: 5.23 s\n",
      "Wall time: 26.8 s\n",
      "VendorID\n",
      "1    2.791033\n",
      "2    3.031729\n",
      "4    2.703563\n",
      "Name: trip_distance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parquet directory size ~ 2.9G\n",
    "%time result = average_result(pq_ddf).compute()    \n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
